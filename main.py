from fastapi import FastAPI
from pydantic import BaseModel
import os, sys, traceback
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from dotenv import load_dotenv
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from pathlib import Path

# ===== í™˜ê²½ =====
BASE_DIR = Path(__file__).resolve().parent
load_dotenv()

HF_AUTH_TOKEN = os.getenv("HF_AUTH_TOKEN")  # Renderì— í™˜ê²½ë³€ìˆ˜ë¡œ ë„£ê¸°
MODEL_NAME = os.getenv("MODEL_NAME", "Junginn/kcelectra-toxic-comment-detector_V1")

# âœ… í† í¬ë‚˜ì´ì €ëŠ” ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ë¶„ë¦¬ (ì—†ìœ¼ë©´ MODEL_NAME ì‚¬ìš©)
TOKENIZER_NAME = os.getenv("TOKENIZER_NAME", "beomi/KcELECTRA-base-v2022")

token_kw = {"token": HF_AUTH_TOKEN} if HF_AUTH_TOKEN else {}

# 1ìˆœìœ„: ë² ì´ìŠ¤ í† í¬ë‚˜ì´ì €(ìŠ¬ë¡œìš°) ì‚¬ìš©
try:
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=False, **token_kw)
except Exception:
    # 2ìˆœìœ„: ë™ì¼ ë¦¬í¬ì—ì„œ fast ì‹œë„ (tokenizer.jsonë§Œ ìˆì„ ë•Œ)
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True, **token_kw)

# ===== ëª¨ë¸ ë¡œë“œ =====
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, **token_kw)
model.tokenizer = tokenizer
model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # RenderëŠ” ë³´í†µ CPU
model.to(device)

app = FastAPI()

class PredictIn(BaseModel):
    text: str

@app.get("/healthz")
def healthz():
    return {"status": "ok"}

# ë””ë²„ê·¸: í˜„ì¬ ìƒíƒœ í™•ì¸ (í† í°/ëª¨ë¸ëª…/í† í¬ë‚˜ì´ì €ëª…)
@app.get("/debug/status")
def debug_status():
    return {
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None,
        "model_name": MODEL_NAME,
        "tokenizer_name": TOKENIZER_NAME,
        "has_token": bool(HF_AUTH_TOKEN),
    }

@app.post("/predict")
def predict(request: PredictIn):
    """
    ì–´ë–¤ ì˜ˆì™¸ê°€ ë‚˜ë„ 200 JSONìœ¼ë¡œ ëŒë ¤ì„œ í”„ë¡ íŠ¸ê°€ í•­ìƒ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„.
    (í•„ìš”í•˜ë©´ status_codeë¥¼ 500ìœ¼ë¡œ ë°”ê¿”ë„ ë˜ì§€ë§Œ, í˜„ì¬ í”„ëŸ°íŠ¸ êµ¬ì¡°ëŠ” 200ì´ í¸í•¨)
    """
    try:
        text = (request.text or "").strip()
        if not text:
            return {
                "ok": True,
                "text": "",
                "predicted_label": 1,
                "label_name": "ì¼ë°˜ ëŒ“ê¸€",
                "probability": 1.0,
                "confidence_color": None,
            }

        inputs = model.tokenizer(
            text,
            return_tensors='pt',
            truncation=True,
            padding='max_length',
            max_length=150,  # (ë©”ëª¨ë¦¬ íƒ€ì´íŠ¸í•˜ë©´ 128ë¡œ ë‚®ì¶”ì„¸ìš”)
            return_attention_mask=True
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1)

        label = int(torch.argmax(probs, dim=-1).item())
        prob = float(probs[0][label].item())
        label_text = 'ì•…í”Œ' if label == 0 else 'ì¼ë°˜ ëŒ“ê¸€'

        # ğŸ”´ í™•ì‹ ë„ ê¸°ë°˜ ìƒ‰ìƒ ì •ì˜
        color = None
        if label == 0:  # ì•…í”Œë¡œ ì˜ˆì¸¡ëœ ê²½ìš°ë§Œ ìƒ‰ìƒ ë¶€ì—¬
            if prob >= 0.65:
                color = "red"
            elif prob >= 0.5:
                color = "orange"
            else:
                # ì•…í”Œë¡œ ì˜ˆì¸¡ëì§€ë§Œ í™•ì‹  ë‚®ìŒ â†’ ì¼ë°˜ìœ¼ë¡œ ê°„ì£¼
                label_text = 'ì¼ë°˜ ëŒ“ê¸€'
                # colorëŠ” None ìœ ì§€

        return {
            "ok": True,
            "text": text,
            "predicted_label": label,
            "label_name": label_text,
            "probability": round(prob, 4),
            "confidence_color": color
        }

    except Exception as e:
        # ì„œë²„ ë¡œê·¸ì— ì›ì¸ ë‚¨ê¸°ê¸°
        print("[/predict ERROR]", e, file=sys.stderr)
        print(traceback.format_exc(), file=sys.stderr)
        # 200ìœ¼ë¡œ JSON ë°˜í™˜(í”„ë¡ íŠ¸ê°€ íŒŒì‹± ê°€ëŠ¥)
        return {
            "ok": False,
            "error": f"{type(e).__name__}: {e}",
            "confidence_color": None
        }

# ===== ì •ì  íŒŒì¼/ë£¨íŠ¸ =====
static_dir = BASE_DIR / "static"
app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

@app.get("/", response_class=HTMLResponse)
def read_index():
    return (static_dir / "index.html").read_text(encoding="utf-8")
